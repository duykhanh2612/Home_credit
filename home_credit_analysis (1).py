# -*- coding: utf-8 -*-
"""home_credit_analysis.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1FL5MBsZkZPSDY7xFPUq7A_teJVSD8q7L

<a href="https://colab.research.google.com/github/vthuhien/House_price/blob/main/Home_credit.ipynb" target="_parent"><img src="https://colab.research.google.com/assets/colab-badge.svg" alt="Open In Colab"/></a>

## About Data
https://www.kaggle.com/competitions/home-credit-default-risk/data </br>
Many people struggle to get loans due to insufficient or non-existent credit histories. And, unfortunately, this population is often taken advantage of by untrustworthy lenders.

Home Credit Group

Home Credit strives to broaden financial inclusion for the unbanked population by providing a positive and safe borrowing experience. In order to make sure this underserved population has a positive loan experience, Home Credit makes use of a variety of alternative data--including telco and transactional information--to predict their clients' repayment abilities.

While Home Credit is currently using various statistical and machine learning methods to make these predictions, they're challenging Kagglers to help them unlock the full potential of their data. Doing so will ensure that clients capable of repayment are not rejected and that loans are given with a principal, maturity, and repayment calendar that will empower their clients to be successful.

### 1. Read Data
I coded directly in Github. The data files are too large so I can't put them into the Git space. For that reason, we must use the Gdown library to read all data files that are linked from Google Drive.</br>
If you need to install the Gdown library, save the code and run it in the terminal within GitHub Codespaces. Here’s the command format for you :</br>
`pip install gdown`</br>
"""

import matplotlib.pyplot as plt
import pandas as pd
import numpy as np
import seaborn as sns
import gdown

file_id = "1K2qAQFxWyC56u9BDptBNkEl4DdL0qrZN"
url = f"https://drive.google.com/uc?id={file_id}"
output = "test_data.csv"

gdown.download(url, output, quiet=False)

df_test = pd.read_csv(output)
df_test

file_id = '1vEEBttpQTsifMQxE4U0_x6MZ7MkkNSV2'
url = f"https://drive.google.com/uc?id={file_id}"
output = "train_data.csv"

gdown.download(url, output, quiet= False)
df_train = pd.read_csv(output)
pd.set_option('display.max_columns', 122)
df_train

"""### 2. Data Overview

#### The Home Credit columns description file</br>The information about each column in dataset that we will be processing. More details in [here](https://drive.google.com/file/d/1lLdrIOLBjj0TYr0rEVpwfoi9CVHojouN/view?usp=drive_link)

SK_ID_CURR,ID of loan in our sample,TARGET,"Target variable (1 - client with payment difficulties: he/she had late payment more than X days on at least one of the first Y installments of the loan in our sample, 0 - all other cases)"</br>
NAME_CONTRACT_TYPE,Identification if loan is cash or revolving,CODE_GENDER,Gender of the client,FLAG_OWN_CAR,Flag if the client owns a car</br>
FLAG_OWN_REALTY,Flag if client owns a house or flat,CNT_CHILDREN,Number of children the client has,</br>
AMT_INCOME_TOTAL,Income of the client,</br>
AMT_CREDIT,Credit amount of the loan,</br>
AMT_ANNUITY,Loan annuity,</br>
AMT_GOODS_PRICE,For consumer loans it is the price of the goods for which the loan is given,
NAME_TYPE_SUITE,Who was accompanying client when he was applying for the loan,</br>
NAME_INCOME_TYPE,"Clients income type (businessman, working, maternity leave,…)",</br>
NAME_EDUCATION_TYPE,Level of highest education the client achieved,</br>
NAME_FAMILY_STATUS,Family status of the client,</br>
NAME_HOUSING_TYPE,"What is the housing situation of the client (renting, living with parents, ...)",</br>
REGION_POPULATION_RELATIVE,Normalized population of region where client lives (higher number means the client lives in more populated region),normalized </br>
DAYS_BIRTH,Client's age in days at the time of application,time only relative to the application</br>
DAYS_EMPLOYED,How many days before the application the person started current employment,time only relative to the application</br>
DAYS_REGISTRATION,How many days before the application did client change his registration,time only relative to the application</br>
"""

df_test.shape

df_train.shape

df_train.dtypes

df_test.describe()

df_train.describe()

"""Based on the description of dataframe train data, we will focus on the `target` variable that evaluates the type of client with payment difficulties. So we will visualize the data by drawing chart to show the tendency data that needs to be resolved. </br>
In `target` variable, we can see the mean value is bigger than the median value so we can guess that the chart will be the right skew.

### 3. Target Distribution
"""

sns.set(style = ('darkgrid'))
sns.histplot(data = df_train['TARGET'], kde = True);

"""Oh, we see the value of 0 is too much more than the value of 1. It indicates that the number of clients with payment difficulties is less than.</br>
Next, we will check each type of variable and the number of unique classes in each object column to see more comprehensively.

"""

df_train.dtypes.value_counts()

df_train.select_dtypes('object').apply(pd.Series.nunique, axis = 0)

"""The `target` is what we are asked to predict : either the value of 0 for the loan was repaid on time, or a 1 indicates that the client with payment difficulties. WWe can first examine the numbers of loans falling into each category."""

df_train['TARGET'].value_counts()

df_train['TARGET'].plot.hist()

"""From this chart, we draw the conclusion that almost all the clients paid the loans in time but still have some customers in value of 1 who have trouble with making payment or late time. The customers with value of 1 just account for 10 percents of all transactions. Since we get into deeper with machine learning models, we can weight the classes by their representations in the data to reflect this imbalance

### 4. Deal with Data
We check each column in the data to clean, remove, keep, or change the values that are missing, error, duplicated, and outliers. That is a useful method for improving the data's clarity and ease of analysis. Now, we will create a table for this.
"""

pd.set_option('display.max_rows',122)
df_train.isnull().sum().sort_values(ascending = False)

def missing_value(df,sort_by = 'percent', limit = 0):
    df_null = pd.DataFrame(df.isnull().sum().reset_index())
    df_null.columns = ['name_col','null_value']

    df_null['percent'] = round((100 * df_null['null_value']/ len(df)),2)
    df_null = df_null[df_null['percent'] > limit]

    df_null = df_null.sort_values(by = sort_by, ascending = False)
    df_null = df_null.reset_index(drop = True)
    return df_null

pd.set_option('display.max_rows', 70)
df_null = missing_value(df_train)
df_null

"""After that, to see more clearly and comprehensively, we visualize the missing values by the bar chart. We can see which column has the most missing value and will deal with values from up to 47% and ignore which values has the minimum percentage from 0% up to 11%"""

plt.figure(figsize=(12, 8))
sns.barplot(y='name_col', x='null_value', data=df_null )
plt.gcf().subplots_adjust(bottom= -1)
# plt.xticks(rotation=90)
plt.show;

"""Look at this bart chart. Because which the dataset we analyze that has too many columns with null values, we must adjust the size of each label and column in chart to fit the screen for easy viewing and simplify analysis. </br>
We give a couple of ways to deal with the columns in the chart. When it comes time to build our machine learning models, we can fill in these missing values. Another option is to remove columns with a high percentage of missing values. However, we found that the columns with a high percentage make up 80% of the total columns. Along with that, we don't know which columns relate to the `target` column that needs to be predicted, so the best choice for us is to keep them.

### 5. Relationships between variables

To handle the columns above, first we will check relationships between the `target` column and the other columns. If which column with missing values relates to the `target` column, we will process them, but if which column has negative relationships with the predict column, we will neglect them.</br>
So, We need to use the heatmap for checking correlation coefficient among columns
"""

import matplotlib.pyplot as plt
import seaborn as sns
numeric_df = df_train.select_dtypes(include=['float64', 'int64'])
value = numeric_df.corr()
k = 40
cols = value.nlargest(k, 'TARGET')['TARGET'].index
plt.figure(figsize=(15, 13))
cm = np.corrcoef(df_train[cols].values.T)
hm = sns.heatmap(cm, cbar=True, annot=True, square=True, annot_kws={'size': 5}, fmt='.2f',yticklabels=cols.values, xticklabels=cols.values)
plt.xticks(fontsize=8)
plt.yticks(fontsize=8)
plt.show()

"""As we see, the value of the other variables is too small to have a correlation relationship with the `target` variable. This chart shows almost all variables only have values from 0.01 to 0.07, and the others have negative values. In this case, we only see 28 variables relates to the variable needs to be analyzed. </br>
However, this chart don't show clearly the correlations of each variable with the `target` so we use the function to express and sort them
"""

# df_new = df_train.drop(columns = ['NAME_CONTRACT_TYPE'])
# df_new
# print(df_train.columns)
numeric = df_train.select_dtypes(include = ['float64', 'int64'])
relationships = numeric.corr()['TARGET'].sort_values()
relationships.tail(20)

"""We can see the results from the chart or the table above that are much smaller than the standard of correlation coefficient. So, we only get the columns with the values from 0.05 to 0.07, the other values are too tiny for analysis.<br>
Accepting `target`, we observe 6 columns that have a positive relationship with the treated column. These are starting at the `REG_CITY_NOT_WORK_CITY` and ending at the `DAYS_BIRTH`. </br>
However, we only get out 3 columns that have high values and are nearest to the `target` column. That are `REGION_RATING_CLIENT`, `REGION_RATING_CLIENT_W_CITY`, `DAYS_BIRTH` columns </br>

Along with that, we also need to check all negative relationships, as they may have an important impact on payment difficulties.
"""

relationships.head(20)

"""We will get out the columns that have the strongest positive correlation or strongest negative correlation to analyze easily.</br>
With this table, we only see 3 columns with the value of -0.1. This indicates that these columns have the most powerful negative correlation compared to the rest of columns.</br>
That are `EXT_SOURCE_3`, `EXT_SOURCE_2`, `EXT_SOURCE_1` columns. So, both affirmative and negative relationships, we have 6 columns related to `target` that need to be processed.

#### 6. Analyze </br>Let's handle 6 columns that we found. First, we start with positive relationship. In this, we will analyze them one by one, starting at `DAYS_BIRTH`, continuing on `REGION_RATING_CLIENT_W_CITY`, ending at `REGION_RATING_CLIEN`

##### `DAYS_BIRTH` </br> Look at the description of data, the `DAYS_BIRTH` means Client's age in days at the time of application,time only relative to the application
"""

df_train['DAYS_BIRTH']

"""When we print the values ​​in `DAYS_BIRTH`, we see that both values ​​are negative. The correlation is positive, but the values of this feature are actually negative, meaning that as the clients get older, they are less likely to default on their loan.</br>
So, we will convert them to positive and check the correlation coefficient again.
"""

df_train['DAYS_BIRTH'] = abs(df_train['DAYS_BIRTH'])
A = df_train['DAYS_BIRTH'] .corr(df_train['TARGET'])
print(A)

"""Not beyond prediction, the result return is negative. This shows that when the clients get older, they tend to repay their loan in time more often. Let's start looking at this variable, we can make a histogram of the age. We will put X axis in years to make a plot with a little more understandable"""

# plt.figure(figsize=(12, 8))
df_train['age'] = df_train['DAYS_BIRTH'].abs()
sns.barplot(y='age', x='TARGET', data=df_train )
plt.title('The distribution of ages')
plt.show;

"""Look at this chart, we couldn't get insight out of this. So, we will change it now."""

sns.set_style('darkgrid')

sns.kdeplot(df_train[df_train['TARGET'] == 0]['DAYS_BIRTH'].abs() / 365, label='target == 0')
sns.kdeplot(df_train[df_train['TARGET'] == 1]['DAYS_BIRTH'].abs() / 365, label='target == 1')

plt.xlabel('Age (years)')
plt.ylabel('Density')
plt.title('The distribution of ages')
plt.legend()
plt.show()

"""By creating a kernel density estimate plot, now this visualization is more clearly and comprehensively for us to comment. The difference between kde plot and histogram plot is the kde plot show out the age in years. First, we filter the D`ata_Train` to select rows where `target` = 0 and `target` = 1. Then, we split them out. After that, we access the `days_birth` column in these filtered rows and divide each value by 365 to convert days into years (age in years). However, all values are negative, so we must change them into positive by absolute.

As we see, In line of `target = 0`, which means the customers who repaid the loans in time have an even distribution density across age groups. However, in line of `target = 1`, which means the client with payment difficulties that is highest in range from the age of 25 to 40 and eases towards 50 to 70. Then, we can point out that older customers are more likely to pay loans well than young customers. With younger customers, they tend to have difficulty in debt repayment because they have many problems to solve but are not yet stable in their jobs, so it forms weak finances.
"""

age_data = df_train[['TARGET','DAYS_BIRTH']]
age_data['years_birth'] = age_data['DAYS_BIRTH'].abs() / 365

age_data['age_binned'] = pd.cut(age_data['years_birth'], bins = np.linspace(20,70, num = 11))
age_new = age_data.groupby('age_binned').mean()
age_new

plt.bar(age_new.index.astype(str), 100*age_new['TARGET'])
plt.xticks(rotation=40)
plt.title('The percentage')
plt.show;

"""The chart return the results that show the percentage of clients who repaid loans late over each age groups. The column from the age of 20 to 25 has about more 12% the number of people have difficulty in paying debt. But, the density starts decreasing towards the right and it only has 4% in last column. Through this chart, we see an inverse correlation between date of birth and debt repayment ability. So, we are more certain about our judgment.

#### `REGION_RATING_CLIENT_W_CITY` : Our rating of the region where client lives with taking city into account (1,2,3)</br>`REGION_RATING_CLIENT` : Our rating of the region where client lives (1,2,3)
"""

value_a = df_train['REGION_RATING_CLIENT'].value_counts()
value_b = df_train['REGION_RATING_CLIENT_W_CITY'].value_counts()

index = sorted(set(value_a.index).union(value_b.index))
value_a = value_a.reindex(index, fill_value= 0)
value_b = value_b.reindex(index, fill_value = 0)

plt.figure(figsize = (10, 8))
bar_width = 0.25
x = np.arange(len(index))

#adjusting each set of bars for 2 columns
plt.bar(x - bar_width/2 ,value_a, width = bar_width, label = 'REGION_RATING_CLIENT')
plt.bar(x + bar_width/2 ,value_b, width = bar_width, label = 'REGION_RATING_CLIENT_W_CITY')
plt.legend()

plt.xticks([ r + bar_width/10 for r in range(len(index))], [1, 2, 3], color = 'red')
plt.title('Compare each value from 2 columns')
plt.show

"""I did't find the discription about each value in these column. But, we can see the value of 2 is too high and has the most value compared to the others. So, I guess 1 means the areas where the customers live have inadequate dwelling conditions and 2 means the regions with medium living conditions, finally 3 means the regions with good qualities."""